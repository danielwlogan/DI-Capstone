{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonal Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "from sklearn import base, linear_model\n",
    "import dill as pickle\n",
    "mpl.rcParams['savefig.dpi'] = 1.5 * mpl.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adjust_date\n",
    "This function takes a pandas DataFrame and artifically adjusts the date column.\n",
    "The default column name is 'date'. The type of date adjustment is defaulted to\n",
    "no change, the user must select year, month or week.\n",
    "\n",
    "    year: set all years to 2000 (a leap year)\n",
    "    month: set all days to 01 (overrides week)\n",
    "    week: set days to either 01, 08, 15 or 22\n",
    "    \n",
    "The date is expected as a string in the form YYYY-MM-DD\n",
    "The function returns the input DataFrame but with the date column adjusted\n",
    "and stored as a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_date(df, date_col='date', year=False, month=False, week=False):\n",
    "    if year: df[date_col] = df[date_col].map(lambda x: x.replace(x[:4], '2000'))\n",
    "    if month: df[date_col] = df[date_col].map(lambda x: x.replace(x[8:], '01'))\n",
    "    if week:\n",
    "        i = 0\n",
    "        for d in df[date_col]:\n",
    "            if int(d[8:]) < 8: new_day = '01'\n",
    "            elif int(d[8:]) < 15: new_day = '08'\n",
    "            elif int(d[8:]) < 22: new_day = '15'\n",
    "            else: new_day = '22'\n",
    "            df.at[i,date_col] = d.replace(d[8:], new_day)\n",
    "            i = i + 1\n",
    "    # Convert 'date' column from strings to a datetimes\n",
    "    df[date_col] = pd.to_datetime(df[date_col], format='%Y-%m-%d')\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "business_datafile = '~/capstone/data/yelp_academic_dataset_business.csv'\n",
    "biz_id = 16  # Column containing the business_id, variable used as dataframe index name\n",
    "user_datafile = '~/capstone/data/yelp_academic_dataset_user.csv'\n",
    "usr_id = 16  # Column containing the user_id, variable used as dataframe index name\n",
    "review_datafile = '~/capstone/data/yelp_academic_dataset_review.csv'\n",
    "rev_id = 1   # Column containing the review_id, variable used as dataframe index name\n",
    "\n",
    "business = pd.read_csv(business_datafile, index_col=biz_id)\n",
    "user = pd.read_csv(user_datafile, index_col=usr_id)\n",
    "review = pd.read_csv(review_datafile, index_col=rev_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trim data to just consider restaurants and convert date column to datetime objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rest_crit = business['categories'].map(lambda x: 'Restaurants' in x)\n",
    "restaurants = business[rest_crit]\n",
    "restaurant_ids = restaurants.index.values\n",
    "rest_reviews = review[review['business_id'].isin(restaurant_ids)]\n",
    "\n",
    "rest_reviews = adjust_date(rest_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print restaurants.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the minimum number of reviews to consider to build the model. Get new DataFrames that have businesses greater than the minimum number of reviews and use that list of restaurants to cull the review DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_reviews = 20\n",
    "\n",
    "rest_train = restaurants[restaurants.review_count >= min_reviews]\n",
    "rest_ids = rest_train.index.values\n",
    "reviews_train = rest_reviews[rest_reviews['business_id'].isin(rest_ids)]\n",
    "reviews_train.drop(['votes.cool','votes.funny','votes.useful','type'], axis=1, inplace=True)\n",
    "reviews_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a model to describe the seasonal variation of reviews. We need to be careful to account for the fact that the number of reviews for a given restaurant are not evenly distributed throughout the year. There maybe more (or less) reviews during times where the reviews will be higher than expected. Building this model requires:\n",
    "\n",
    "1. Finding the mean star rating of each restaurant *independent* of the number of reviews. This is accomplished through grouping the review DataFrame by both business_id and date and taking the mean.\n",
    "2. Given a DataFrame grouped by business_id and date, we need to group again by business_id to get a final mean rating for a restaurant. This final mean rating should be a closer approximation to the time independent rating of the restaurant. However, if the number of reviews are small or happen during high/low times, it might be off.\n",
    "3. Next, subtract this new mean rating (from the previous step) from the second grouped by DataFrame (from the previous step) to create a new DataFrame of mean adjusted ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_reviews = reviews_train.groupby(['business_id','date'], as_index=False).mean()\n",
    "grouped_reviews.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_per_rest = grouped_reviews.groupby('business_id', as_index=False).mean()\n",
    "avg_per_rest.rename(columns = {'stars':'avg_stars'}, inplace = True)\n",
    "avg_per_rest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really necessary, but good to know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_all_rest = avg_per_rest[\"avg_stars\"].mean()\n",
    "print 'Average star rating of all the restaurants: {}'.format(avg_all_rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our assumption that seasonal adjustments affect the ratings is correct, then we expect to see similiar adjustements per metro area. We need to add the metro area to reviews DataFrame. As a proxy for metro area, we will use state. We must make allowances for those metro areas which have multiple states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = restaurants.reset_index()\n",
    "state = tmp[['business_id','state']]\n",
    "print state.state.unique()\n",
    "# Combine multi-state metro areas to a single state metro-area\n",
    "# Drop the clearly mislabeled/bad data\n",
    "# XGL - is the code for greater London\n",
    "# NW - is the code for Nordrhein-Westfalen, not near Karlsrhue\n",
    "state.state.replace(to_replace=['SC','MLN','FIF','ELN','BW','RP'],\n",
    "                    value=['NC','EDH','EDH','EDH','KHL','KHL'],\n",
    "                    inplace=True)\n",
    "state = state[state.state != 'XGL']\n",
    "state = state[state.state != 'NW']\n",
    "print state.state.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_reviews = pd.merge(grouped_reviews, avg_per_rest, on='business_id')\n",
    "grouped_reviews['stars_adj'] = grouped_reviews.stars - grouped_reviews.avg_stars\n",
    "grouped_reviews = pd.merge(grouped_reviews, state, on='business_id')\n",
    "grouped_reviews.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arizona\n",
    "\n",
    "Added cutoff date to make sure that we have a review on every day. I am doing this so I can run an FFT without worrying about missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AZ = grouped_reviews[grouped_reviews['state'] == 'AZ'].groupby('date', as_index=False).agg(['mean', 'std', 'count'])\n",
    "cutoff_date = dt.date(2008, 1, 1)\n",
    "AZ = grouped_reviews[(grouped_reviews.state == 'AZ') & (grouped_reviews.date > cutoff_date)]\n",
    "AZ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AZ_gb_date = AZ.groupby('date', as_index=False).mean()\n",
    "AZ_gb_date.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(AZ_gb_date.date, AZ_gb_date.stars_adj, 'b.', label='data', alpha=0.5)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Star Rating')\n",
    "plt.legend(loc='upper right')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.abs(np.fft.rfft(AZ_gb_date.stars_adj))\n",
    "\n",
    "n = AZ_gb_date.stars_adj.size\n",
    "timestep = 1.\n",
    "x = np.fft.rfftfreq(n, d=timestep)\n",
    "plt.title(\"PSD of ratings AZ adjusted stars\")\n",
    "plt.xlabel(\"Frequency [cycles/day]\")\n",
    "plt.ylabel(\"Power\")\n",
    "plt.plot(x, y**2)\n",
    "plt.show()\n",
    "\n",
    "xran = [.1,.2]\n",
    "#nlabels = 8\n",
    "#xlabels = np.arange(nlabels)*(xran[1]-xran[0])+xran[0]\n",
    "plt.plot(x, y**2, color='dodgerblue', alpha=0.5)\n",
    "plt.ylim([0,800])\n",
    "#plt.xlim([150.,1450.])\n",
    "plt.xlim(xran)\n",
    "#plt.xticks(range(nlabels),xlabels)\n",
    "plt.title(\"PSD of Arizona Restaurants\")\n",
    "plt.ylabel(\"Power\")\n",
    "plt.xlabel(\"Frequency [cycles/day]\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, y**2)\n",
    "plt.ylim([0,800])\n",
    "plt.xlim([0,.04])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1 = np.argmax(y[1:1000])+1\n",
    "print f1, x[f1]\n",
    "print 'peak: {} cycles/week'.format(x[f1]*7.)\n",
    "f2 = np.argmax(y[:200])\n",
    "print f2, x[f2]\n",
    "print 'peak: {} cycles/year'.format(x[f2]*365.25)\n",
    "f3 = np.argmax(y[f2+1:200])+f2+1\n",
    "print f3, x[f3]\n",
    "print 'peak: {} cycles/year'.format(x[f3]*365.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pittsburgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cutoff_date = dt.date(2008, 1, 1)\n",
    "PA = grouped_reviews[(grouped_reviews.state == 'PA') & (grouped_reviews.date > cutoff_date)]\n",
    "PA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PA_gb_date = PA.groupby('date', as_index=False).mean()\n",
    "PA_gb_date.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(PA_gb_date.date, PA_gb_date.stars_adj, 'b.', label='data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Star Rating')\n",
    "plt.legend(loc='upper right')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.abs(np.fft.rfft(PA_gb_date.stars_adj))\n",
    "\n",
    "n = PA_gb_date.stars_adj.size\n",
    "timestep = 1.\n",
    "x = np.fft.rfftfreq(n, d=timestep)\n",
    "\n",
    "plt.title(\"PSD of ratings PA adjusted stars\")\n",
    "plt.plot(x, y**2)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, y**2)\n",
    "plt.ylim([0,4000])\n",
    "plt.xlim([.1,.2])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, y**2)\n",
    "plt.ylim([0,4000])\n",
    "plt.xlim([0,.04])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1 = np.argmax(y[1:4000])+1\n",
    "print f1, x[f1]\n",
    "print 'peak: {} cycles/week'.format(x[f1]*7.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_freq = [x[i] for i,v in enumerate(y[:100]**2) if v > 2100]\n",
    "for f in top_freq:\n",
    "    print 'peak: {} cycles/year'.format(f*365.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Las Vegas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cutoff_date = dt.date(2008, 1, 1)\n",
    "NV = grouped_reviews[(grouped_reviews.state == 'NV') & (grouped_reviews.date > cutoff_date)]\n",
    "NV.head()\n",
    "NV_gb_date = NV.groupby('date', as_index=False).mean()\n",
    "NV_gb_date.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(NV_gb_date.date, NV_gb_date.stars_adj, 'b.', label='data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Star Rating')\n",
    "plt.legend(loc='upper right')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.abs(np.fft.rfft(NV_gb_date.stars_adj))\n",
    "\n",
    "n = NV_gb_date.stars_adj.size\n",
    "timestep = 1.\n",
    "x = np.fft.rfftfreq(n, d=timestep)\n",
    "\n",
    "plt.title(\"PSD of ratings NV adjusted stars\")\n",
    "plt.plot(x, y**2)\n",
    "plt.ylim([0,500])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, y**2)\n",
    "plt.ylim([0,500])\n",
    "plt.xlim([.1,.2])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, y**2)\n",
    "plt.ylim([0,500])\n",
    "plt.xlim([0,.04])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1 = np.argmax(y[100:1000])+100\n",
    "print f1, x[f1]\n",
    "print 'peak: {} cycles/week'.format(x[f1]*7.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_freq = [x[i] for i,v in enumerate(y[:1000]**2) if v > 300]\n",
    "for f in top_freq:\n",
    "    print 'peak: {} cycles/year'.format(f*365.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### What's the take away message?\n",
    "\n",
    "It is clear there is some cyclical variation on both annual and weekly cycles. There maybe more funky issues that come up on an annual (monthly?) cycle that is not a simple sine/cosine with a period of one year. (Think a step function for a few months) Pressing ahead, I am going to go back to all the data and create new features of the sine/cosine by the year and week. Next, I will fit those new features to a per city model and store the model. This model will give me the expected change in star rating given the date. I can then apply this model to all of my restaurants and get an adjustment for every review. Get the average of that to come up with an average correction based on the date.\n",
    "\n",
    "The next step of the model would be to investigate if the adjustments were related to sun or temperature or something else instead of a general day of the year thing. I would have to get sun and temperature data for each city to build a better(?) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add columns to DataFrame for new features\n",
    "new_feat = grouped_reviews.copy()\n",
    "new_feat['day'] = grouped_reviews.date.apply(lambda x: x.dayofyear)\n",
    "new_feat['day_of_week'] = grouped_reviews.date.apply(lambda x: x.dayofweek)\n",
    "new_feat['sin(day)'] = np.sin(new_feat['day'] / 365. * 2. * np.pi)\n",
    "new_feat['cos(day)'] = np.cos(new_feat['day'] / 365. * 2. * np.pi)\n",
    "new_feat['sin(week)'] = np.sin(new_feat['day_of_week'] / 7. * 2. * np.pi)\n",
    "new_feat['cos(week)'] = np.cos(new_feat['day_of_week'] / 7. * 2. * np.pi)\n",
    "new_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DateModel(base.BaseEstimator, base.RegressorMixin):\n",
    "    def __init__(self, estimator):\n",
    "        # initialization code\n",
    "        self.est = estimator\n",
    "        self.func = {}\n",
    "        return\n",
    "        \n",
    "    def fit(self, X, Y=None):\n",
    "        # fit the model ..-\n",
    "        for state, df in X.groupby(['state'], sort=False):\n",
    "            linreg = self.est()\n",
    "            my_est = linreg.fit(df[['sin(day)', 'cos(day)', 'sin(week)', 'cos(week)']], df['stars_adj'])\n",
    "            self.func[state] = my_est\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # prediction\n",
    "        key = X['state']\n",
    "        if key in self.func.keys():\n",
    "            if isinstance(X,dict):\n",
    "                return self.func.get(key).predict([X['sin(day)'], X['cos(day)'],\n",
    "                                                   X['sin(week)'], X['cos(week)']])\n",
    "            else:\n",
    "                return self.func.get(key).predict(X[['sin(day)', 'cos(day)', 'sin(week)', 'cos(week)']])\n",
    "        else:\n",
    "            return -9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_date_model = DateModel(linear_model.Ridge)\n",
    "my_date_model.fit(new_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 3000\n",
    "X = new_feat.iloc[N]\n",
    "Y = X['stars_adj']\n",
    "\n",
    "print 'Prediction = {}, Actual rating = {}'.format(my_date_model.predict(X)[0],Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = '/home/vagrant/capstone/data/date_model.pkl'\n",
    "with open(filename,'w') as f:\n",
    "    pickle.dump(my_date_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rest_reviews = review[review['business_id'].isin(restaurant_ids)].copy()\n",
    "rest_reviews = adjust_date(rest_reviews)\n",
    "rest_reviews.drop(['votes.cool','votes.funny','votes.useful','type'], axis=1, inplace=True)\n",
    "rest_reviews_state_corrected = pd.merge(rest_reviews, state, on='business_id')\n",
    "\n",
    "# Add columns to DataFrame for new features\n",
    "rest_views = rest_reviews_state_corrected.copy()\n",
    "rest_views['day'] = rest_reviews_state_corrected.date.apply(lambda x: x.dayofyear)\n",
    "rest_views['day_of_week'] = rest_reviews_state_corrected.date.apply(lambda x: x.dayofweek)\n",
    "rest_views['sin(day)'] = np.sin(rest_views['day'] / 365. * 2. * np.pi)\n",
    "rest_views['cos(day)'] = np.cos(rest_views['day'] / 365. * 2. * np.pi)\n",
    "rest_views['sin(week)'] = np.sin(rest_views['day_of_week'] / 7. * 2. * np.pi)\n",
    "rest_views['cos(week)'] = np.cos(rest_views['day_of_week'] / 7. * 2. * np.pi)\n",
    "rest_views.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dumb(x):\n",
    "    return my_date_model.predict(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_corrected = rest_views.apply(lambda x: my_date_model.predict(x)[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_corrected_reviews = rest_views.copy()\n",
    "model_corrected_reviews['date_correction'] = model_corrected\n",
    "model_corrected_reviews.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = '/home/vagrant/capstone/data/model_corrected_reviews.pkl'\n",
    "with open(filename,'w') as f:\n",
    "    pickle.dump(model_corrected_reviews, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = '/home/vagrant/capstone/data/model_corrected_reviews.pkl'\n",
    "with open(filename,'r') as f:\n",
    "    model_corrected_reviews = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_mc_reviews = model_corrected_reviews.groupby('business_id', as_index=False).mean()\n",
    "mean_mc_reviews.drop(['day','day_of_week','sin(day)','cos(day)','sin(week)','cos(week)'], axis=1, inplace=True)\n",
    "mean_mc_reviews['date_corrected_stars'] = mean_mc_reviews.stars - mean_mc_reviews.date_correction\n",
    "mean_mc_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'The maximum restaurant OVER-rating is {}'.format(mean_mc_reviews.date_correction.max())\n",
    "print 'The maximum restaurant UNDER-rating is {}'.format(mean_mc_reviews.date_correction.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '/home/vagrant/capstone/data/date_adjusted_ratings.pkl'\n",
    "with open(filename,'w') as f:\n",
    "    pickle.dump(mean_mc_reviews, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
